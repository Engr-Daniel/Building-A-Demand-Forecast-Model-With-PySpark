{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6918e18a-c248-4929-b552-7aee2057c0eb",
   "metadata": {},
   "source": [
    "![Shopping trolley in front of a laptop](./iStock-1249219777.jpg)\n",
    "\n",
    "It's simple to buy any product with a click and have it delivered to your door. Online shopping has been rapidly evolving over the last few years, making our lives easier. But behind the scenes, e-commerce companies face a complex challenge that needs to be addressed. \n",
    "\n",
    "Uncertainty plays a big role in how the supply chains plan and organize their operations to ensure that the products are delivered on time. These uncertainties can lead to challenges such as stockouts, delayed deliveries, and increased operational costs.\n",
    "\n",
    "**Problem Statement:**\n",
    "\n",
    "You work for the Sales & Operations Planning (S&OP) team at a multinational e-commerce company. They need your help to assist in planning for the upcoming end-of-the-year sales. They want to use your insights to plan for promotional opportunities and manage their inventory. This effort is to ensure they have the right products in stock when needed and ensure their customers are satisfied with the prompt delivery to their doorstep.\n",
    "\n",
    "\n",
    "## The Data\n",
    "\n",
    "You are provided with a sales dataset to use. A summary and preview are provided below.\n",
    "\n",
    "# Online Retail.csv\n",
    "\n",
    "| Column     | Description              |\n",
    "|------------|--------------------------|\n",
    "| `'InvoiceNo'` | A 6-digit number uniquely assigned to each transaction |\n",
    "| `'StockCode'` | A 5-digit number uniquely assigned to each distinct product |\n",
    "| `'Description'` | The product name |\n",
    "| `'Quantity'` | The quantity of each product (item) per transaction |\n",
    "| `'UnitPrice'` | Product price per unit |\n",
    "| `'CustomerID'` | A 5-digit number uniquely assigned to each customer |\n",
    "| `'Country'` | The name of the country where each customer resides |\n",
    "| `'InvoiceDate'` | The day and time when each transaction was generated `\"MM/DD/YYYY\"` |\n",
    "| `'Year'` | The year when each transaction was generated |\n",
    "| `'Month'` | The month when each transaction was generated |\n",
    "| `'Week'` | The week when each transaction was generated (`1`-`52`) |\n",
    "| `'Day'` | The day of the month when each transaction was generated (`1`-`31`) |\n",
    "| `'DayOfWeek'` | The day of the weeke when each transaction was generated <br>(`0` = Monday, `6` = Sunday) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28236a4",
   "metadata": {},
   "source": [
    "## Install pyspark\n",
    "\n",
    "PySpark is a Python library that provides an interface for Apache Spark, a powerful distributed computing framework. Installing PySpark via pip ensures that all necessary dependencies are fetched and installed on your system.\n",
    "\n",
    "Without installing PySpark, we won't be able to use its functionalities in our Python environment. Installing it using pip makes it accessible for importing into the notebook."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a4ffd93",
   "metadata": {},
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e1401",
   "metadata": {},
   "source": [
    "## Import Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783f0d75",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1711380760819,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required libraries \n\n#Importing SparkSession for Spark functionality:\n#This line imports the SparkSession class from the pyspark.sql module. \n#SparkSession is the entry point to Spark functionality in PySpark applications\nfrom pyspark.sql import SparkSession\n\n# Importing VectorAssembler for assembling feature vectors\n#VectorAssembler is used to assemble vectors of features, which are required for machine learning algorithms in PySpark.\nfrom pyspark.ml.feature import VectorAssembler\n\n# Importing Pipeline for building data processing pipelines\n#Pipelines are a sequence of stages used to process and transform data in PySpark, \n#commonly used for building machine learning workflows.\nfrom pyspark.ml import Pipeline\n\n# Importing  DecisionTreeRegressor for regression modeling\n#DecisionTreeRegressor is a machine learning algorithm used for regression tasks, such as demand forecasting\nfrom pyspark.ml.regression import DecisionTreeRegressor\n\n# Importing various functions for data manipulation\n#These functions are commonly used for data manipulation and feature engineering tasks in PySpark.\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n\n# Importing StringIndexer for converting categorical variables\n#StringIndexer is used to convert categorical variables into numerical format.\nfrom pyspark.ml.feature import StringIndexer\n\n# Importing RegressionEvaluator for evaluating regression model performance\n#RegressionEvaluator is used to evaluate the performance of regression models, \n#providing metrics such as RMSE (Root Mean Squared Error) or R^2 (Coefficient of Determination).\nfrom pyspark.ml.evaluation import RegressionEvaluator"
   },
   "outputs": [],
   "source": [
    "# Import required libraries \n",
    "\n",
    "#Importing SparkSession for Spark functionality:\n",
    "#This line imports the SparkSession class from the pyspark.sql module. \n",
    "#SparkSession is the entry point to Spark functionality in PySpark applications\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importing VectorAssembler for assembling feature vectors\n",
    "#VectorAssembler is used to assemble vectors of features, which are required for machine learning algorithms in PySpark.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Importing Pipeline for building data processing pipelines\n",
    "#Pipelines are a sequence of stages used to process and transform data in PySpark, \n",
    "#commonly used for building machine learning workflows.\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Importing  DecisionTreeRegressor for regression modeling\n",
    "#DecisionTreeRegressor is a machine learning algorithm used for regression tasks, such as demand forecasting\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Importing various functions for data manipulation\n",
    "#These functions are commonly used for data manipulation and feature engineering tasks in PySpark.\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n",
    "\n",
    "# Importing StringIndexer for converting categorical variables\n",
    "#StringIndexer is used to convert categorical variables into numerical format.\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Importing RegressionEvaluator for evaluating regression model performance\n",
    "#RegressionEvaluator is used to evaluate the performance of regression models, \n",
    "#providing metrics such as RMSE (Root Mean Squared Error) or R^2 (Coefficient of Determination).\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854d1c8",
   "metadata": {},
   "source": [
    "## Initialize Spark session\n",
    "\n",
    "- **Initialize Spark session**: This line initializes a SparkSession object named my_spark. SparkSession is the entry point to Spark functionality in PySpark applications. It provides a way to interact with Spark and allows you to create DataFrames, execute SQL queries, and work with machine learning algorithms.\n",
    "\n",
    "- **SparkSession.builder**: This starts the process of creating a SparkSession. The builder() method returns a Builder object which is used to configure various SparkSession options.\n",
    "\n",
    "- **appName(\"SalesForecast\")**: This sets the name of the Spark application to \"SalesForecast\". The name is used to identify your application on the Spark cluster's UI (User Interface) and in logs.\n",
    "\n",
    "- **getOrCreate()**: This method tries to reuse an existing SparkSession if it exists or creates a new one if none exists. This ensures that you have a SparkSession available for your application to use, promoting efficient resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbaa028c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2515,
    "lastExecutedAt": 1711380365438,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     },
     "1": {
      "height": 57,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/25 15:26:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "my_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9016e-beec-4db1-920a-69f6bc680ace",
   "metadata": {},
   "source": [
    "## Import the Data\n",
    "\n",
    "- **my_spark.read.csv()**: This is a method provided by the SparkSession object (my_spark) to read data from various sources, in this case, a CSV file.\n",
    "\n",
    "- **\"Online Retail.csv\"**: This is the path to the CSV file containing the sales data. Spark will look for this file in the current working directory unless an absolute path is provided.\n",
    "\n",
    "- **header=True**: This parameter indicates that the first row of the CSV file contains the column names. Setting header=True ensures that Spark uses the first row as the header, assigning column names accordingly.\n",
    "\n",
    "- **inferSchema=True**: This parameter instructs Spark to infer the data types of each column in the CSV file. Spark will automatically analyze a sample of the data to determine the appropriate data types (e.g., integer, string, float) for each column.\n",
    "\n",
    "- **sep=\",\"**: This parameter specifies the delimiter used in the CSV file. In this case, a comma (\",\") is used as the delimiter. Spark will split the data in each row based on this delimiter to create columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa892bcd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3919,
    "lastExecutedAt": 1711380373822,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Importing sales data\n#This line imports sales data from a CSV file named \"Online Retail.csv\" into a DataFrame named sales_data.\n\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\nsales_data.head()",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(InvoiceNo=536365, StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, UnitPrice=2.55, CustomerID=17850, Country='United Kingdom', InvoiceDate=datetime.datetime(2010, 1, 12, 8, 26), Year=2010, Month=1, Week=2, Day=12, DayOfWeek=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing sales data\n",
    "#This line imports sales data from a CSV file named \"Online Retail.csv\" into a DataFrame named sales_data.\n",
    "\n",
    "sales_data = my_spark.read.csv(\n",
    "    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "\n",
    "# Show the first row \n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb49bc2-a013-4878-b0ec-39ebeddee6c8",
   "metadata": {},
   "source": [
    "### - Convert the InvoiceDate to datetime\n",
    "\n",
    "**sales_data.withColumn(\"InvoiceDate\", ...)**: This is a DataFrame method in PySpark used to add or replace a column named \"InvoiceDate\" in the sales_data DataFrame.\n",
    "\n",
    "**to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")**: This part of the code converts the \"InvoiceDate\" column from string format to a timestamp format. It uses the to_timestamp() function, which converts a string to a timestamp, with the format specified as \"d/M/yyyy H:mm\". This format indicates that the date is in day/month/year hour:minute format.\n",
    "\n",
    "**to_date(...)**: This function converts the timestamp obtained from to_timestamp() to a date format, discarding the time information. It ensures that only the date part remains in the \"InvoiceDate\" column.\n",
    "\n",
    "**col(\"InvoiceDate\")**: This is used to reference the \"InvoiceDate\" column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de3f41f-e596-4e89-8e35-90d6951e9233",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 147,
    "lastExecutedAt": 1711380401382,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n\n# shows the first row of the DataFrame sales_data\nsales_data.head()"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(InvoiceNo=536365, StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, UnitPrice=2.55, CustomerID=17850, Country='United Kingdom', InvoiceDate=datetime.date(2010, 1, 12), Year=2010, Month=1, Week=2, Day=12, DayOfWeek=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert InvoiceDate to datetime \n",
    "sales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n",
    "    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n",
    "\n",
    "# shows the first row of the DataFrame sales_data\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e6c92-3d9c-4843-b2fd-de72b60625bc",
   "metadata": {},
   "source": [
    "### Aggregate the Data into Daily Intervals\n",
    "\n",
    "- **sales_data.groupBy(...)**: This groups the data in the sales_data DataFrame based on specified columns, which include \"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", and \"DayOfWeek\".\n",
    "\n",
    "- **.agg({\"Quantity\": \"sum\", \"UnitPrice\": \"avg\"})**: This performs aggregation functions on selected columns after grouping. In this case:\n",
    "\n",
    "- **\"Quantity\"**: \"sum\" calculates the sum of the \"Quantity\" column for each group, aggregating the total quantity of items sold.\n",
    "\n",
    "- **\"UnitPrice\"**: \"avg\" calculates the average of the \"UnitPrice\" column for each group, aggregating the average unit price of items sold.\n",
    "daily_sales_data: This assigns the result of the aggregation operation to a new DataFrame named daily_sales_data.\n",
    "\n",
    "This aggregated data can be useful for further analysis and modeling, particularly for understanding sales trends over time and in different regions or for forecasting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a699d442-fe9e-468b-832c-5820f4494c33",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1667,
    "lastExecutedAt": 1711380411578,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Aggregate data into daily intervals\ndaily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\", \"UnitPrice\": \"avg\"})\n\ndaily_sales_data.head()",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Country='United Kingdom', StockCode='22912', InvoiceDate=datetime.date(2010, 1, 12), Year=2010, Month=1, Day=12, Week=2, DayOfWeek=1, avg(UnitPrice)=4.95, sum(Quantity)=3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate data into daily intervals\n",
    "daily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\", \"UnitPrice\": \"avg\"})\n",
    "\n",
    "daily_sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db90f4f-4a47-45a2-99e0-6af52c7d5a88",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 614,
    "lastExecutedAt": 1711380419296,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Rename the target column\ndaily_sales_data = daily_sales_data.withColumnRenamed(\n    \"sum(Quantity)\", \"Quantity\")\n\ndaily_sales_data.head()"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Country='United Kingdom', StockCode='22912', InvoiceDate=datetime.date(2010, 1, 12), Year=2010, Month=1, Day=12, Week=2, DayOfWeek=1, avg(UnitPrice)=4.95, Quantity=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the target column\n",
    "daily_sales_data = daily_sales_data.withColumnRenamed(\n",
    "    \"sum(Quantity)\", \"Quantity\")\n",
    "\n",
    "daily_sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134b8944-a42b-4c3d-81e9-7ba0096b0e4a",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "\n",
    "Split the data into two sets based on the spliting date, \"2011-09-25\". All data up to and including this date should be in the training set, while data after this date should be in the testing set. Return a pandas Dataframe, pd_daily_train_data, containing, at least, the columns [\"Country\", \"StockCode\", \"InvoiceDate\", \"Quantity\"].\n",
    "\n",
    "These operations split the data into two datasets: one for training (up to and including the splitting date) and one for testing (after the splitting date), allowing for the development and evaluation of predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cd385a-f794-485a-9b31-fdd2256a193b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 23,
    "lastExecutedAt": 1711380476417,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# initiate the date for splitting\nsplit_date_train_test = \"2011-09-25\"\n\n# Creating the train and test datasets\ntrain_data = daily_sales_data.filter(\n    col(\"InvoiceDate\") <= split_date_train_test)\n\ntest_data = daily_sales_data.filter(\n    col(\"InvoiceDate\") > split_date_train_test)"
   },
   "outputs": [],
   "source": [
    "# initiate the date for splitting\n",
    "split_date_train_test = \"2011-09-25\"\n",
    "\n",
    "# Creating the train and test datasets\n",
    "train_data = daily_sales_data.filter(\n",
    "    col(\"InvoiceDate\") <= split_date_train_test)\n",
    "\n",
    "test_data = daily_sales_data.filter(\n",
    "    col(\"InvoiceDate\") > split_date_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4730e4-e27b-4837-a387-2e93e200909a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2333,
    "lastExecutedAt": 1711380485831,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# converts the PySpark DataFrame train_data into a pandas DataFrame pd_daily_train_data\npd_daily_train_data = train_data.toPandas()\n\n#Check the number of columns and rows for the train data\nprint(pd_daily_train_data.shape)\n\n#displays the first few rows of the pandas DataFrame\npd_daily_train_data.head()",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     },
     "1": {
      "height": 211,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175452, 10)\n"
     ]
    },
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "Country": [
          "United Kingdom",
          "France",
          "United Kingdom",
          "United Kingdom",
          "Norway"
         ],
         "Day": [
          12,
          12,
          12,
          12,
          12
         ],
         "DayOfWeek": [
          1,
          1,
          1,
          1,
          1
         ],
         "InvoiceDate": [
          "2010-01-12T00:00:00.000",
          "2010-01-12T00:00:00.000",
          "2010-01-12T00:00:00.000",
          "2010-01-12T00:00:00.000",
          "2010-01-12T00:00:00.000"
         ],
         "Month": [
          1,
          1,
          1,
          1,
          1
         ],
         "Quantity": [
          3,
          24,
          12,
          16,
          12
         ],
         "StockCode": [
          "22912",
          "22659",
          "21544",
          "21098",
          "85150"
         ],
         "Week": [
          2,
          2,
          2,
          2,
          2
         ],
         "Year": [
          2010,
          2010,
          2010,
          2010,
          2010
         ],
         "avg(UnitPrice)": [
          4.95,
          1.95,
          0.85,
          1.25,
          2.55
         ],
         "index": [
          0,
          1,
          2,
          3,
          4
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "Country",
           "type": "string"
          },
          {
           "name": "StockCode",
           "type": "string"
          },
          {
           "name": "InvoiceDate",
           "type": "string"
          },
          {
           "name": "Year",
           "type": "integer"
          },
          {
           "name": "Month",
           "type": "integer"
          },
          {
           "name": "Day",
           "type": "integer"
          },
          {
           "name": "Week",
           "type": "integer"
          },
          {
           "name": "DayOfWeek",
           "type": "integer"
          },
          {
           "name": "avg(UnitPrice)",
           "type": "number"
          },
          {
           "name": "Quantity",
           "type": "integer"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 5,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Week</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>avg(UnitPrice)</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>22912</td>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.95</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>22659</td>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>21544</td>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>21098</td>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Norway</td>\n",
       "      <td>85150</td>\n",
       "      <td>2010-01-12</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country StockCode InvoiceDate  ...  DayOfWeek  avg(UnitPrice)  Quantity\n",
       "0  United Kingdom     22912  2010-01-12  ...          1            4.95         3\n",
       "1          France     22659  2010-01-12  ...          1            1.95        24\n",
       "2  United Kingdom     21544  2010-01-12  ...          1            0.85        12\n",
       "3  United Kingdom     21098  2010-01-12  ...          1            1.25        16\n",
       "4          Norway     85150  2010-01-12  ...          1            2.55        12\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converts the PySpark DataFrame train_data into a pandas DataFrame pd_daily_train_data\n",
    "pd_daily_train_data = train_data.toPandas()\n",
    "\n",
    "#Check the number of columns and rows for the train data\n",
    "print(pd_daily_train_data.shape)\n",
    "\n",
    "#displays the first few rows of the pandas DataFrame\n",
    "pd_daily_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcfc3743-cd18-447a-a7a3-efa8bbb43d8a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1711380493619,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Checking data types\n\ndaily_sales_data"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Country: string, StockCode: string, InvoiceDate: date, Year: int, Month: int, Day: int, Week: int, DayOfWeek: int, avg(UnitPrice): double, Quantity: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking data types\n",
    "\n",
    "daily_sales_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df33c6f-59f7-44dc-acca-ebc1a5a8e6b6",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6b5cc-33de-456f-99a6-81081ecfc6d9",
   "metadata": {},
   "source": [
    "- ## Creating indexer for categorical columns\n",
    "\n",
    "**StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\")**: This creates a StringIndexer instance for the \"Country\" column. The inputCol parameter specifies the input column name, and the outputCol parameter specifies the name of the output column that will contain the indexed values. The indexed values are typically numeric representations of the original categorical values.\n",
    "\n",
    "**.setHandleInvalid(\"keep\")**: This sets the strategy to handle invalid input values during indexing. In this case, \"keep\" strategy is used, which means that if an unseen label is encountered during indexing (i.e., a label that was not present during fitting), it will be kept and assigned a new index.\n",
    "\n",
    "**StringIndexer(inputCol=\"StockCode\", outputCol=\"StockCodeIndex\")**: Similarly, this creates a StringIndexer instance for the \"StockCode\" column with the output column named \"StockCodeIndex\"\n",
    "\n",
    "These indexers are used to convert categorical columns into numerical representations, which is often necessary for machine learning algorithms that require numeric input. By indexing categorical columns, we ensure that the model can process these columns effectively during training and prediction phases.\n",
    "\n",
    "- ## Selecting features columns\n",
    "\n",
    "These features are chosen based on domain knowledge and/or feature importance analysis to capture relevant information for predicting sales quantity or other target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad15900-fc8f-41af-ae4c-a42972093ea9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 24,
    "lastExecutedAt": 1711380505187,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Creating indexer for categorical columns\ncountry_indexer = StringIndexer(\n    inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\nstock_code_indexer = StringIndexer(\n    inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")"
   },
   "outputs": [],
   "source": [
    "# Creating indexer for categorical columns\n",
    "country_indexer = StringIndexer(\n",
    "    inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\n",
    "stock_code_indexer = StringIndexer(\n",
    "    inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab90fca-24ae-4057-be02-8981830e40d7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1711380512242,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Selectiong features columns\nfeature_cols = [\"CountryIndex\", \"StockCodeIndex\", \"Month\", \"Year\",\n                \"DayOfWeek\", \"Day\", \"Week\"]"
   },
   "outputs": [],
   "source": [
    "# Selectiong features columns\n",
    "feature_cols = [\"CountryIndex\", \"StockCodeIndex\", \"Month\", \"Year\",\n",
    "                \"DayOfWeek\", \"Day\", \"Week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc96549-7857-41fe-a1ba-9f4b60124be6",
   "metadata": {},
   "source": [
    "- ## Using Vector assembler to combine features\n",
    "\n",
    "**assembler**: This variable represents a VectorAssembler instance, which is used to assemble the selected feature columns into a single vector column.\n",
    "\n",
    "**VectorAssembler**: This is a transformer provided by PySpark's MLlib library. It combines multiple columns of features into a single vector column.\n",
    "\n",
    "**inputCols=feature_cols**: This parameter specifies the names of the input feature columns that should be combined. In this case, feature_cols is a list of column names containing the selected features.\n",
    "\n",
    "**outputCol=\"features\"**: This parameter specifies the name of the output vector column that will contain the assembled feature vectors. The column name \"features\" is a common convention used in PySpark.\n",
    "\n",
    "By using the VectorAssembler, the individual feature columns specified in feature_cols are combined into a single vector column named \"features\", which can then be used as input for machine learning algorithms in PySpark. This consolidated format is often required by machine learning algorithms to process the data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79132857-dbae-4499-8e2a-c68dab3994bb",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1711380519822,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Using vector assembler to combine features\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   },
   "outputs": [],
   "source": [
    "# Using vector assembler to combine features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f293e04-502a-4ff2-a3a1-b193077d96dc",
   "metadata": {},
   "source": [
    "## Building The Regression Model\n",
    "\n",
    "from the model selection test, I conclude decision trre to be best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c82c63-4f9b-48ca-bf9e-5b798babb4b0",
   "metadata": {},
   "source": [
    "- ## Initializing a Decision Tree model\n",
    "\n",
    "**featuresCol=\"features\"**: Specifies the name of the input column containing the features used for training the decision tree. In this case, it's set to \"features\", which is a common convention for the column name produced by the VectorAssembler when assembling feature vectors.\n",
    "\n",
    "**labelCol=\"Quantity\"**: Specifies the name of the column containing the target variable (the quantity being predicted). In this case, it's set to \"Quantity\", indicating that the decision tree should predict the quantity based on the features.\n",
    "\n",
    "**maxBins=4000**: Specifies the maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. Binning is a process used to discretize continuous features into categorical features, which is necessary for decision trees. Setting maxBins to a higher value can help improve model accuracy, especially when dealing with datasets with many distinct values for continuous features.\n",
    "\n",
    "By initializing the DecisionTreeRegressor object with these parameters, you're configuring it to train a decision tree regression model using the specified features and target variable, and controlling the binning process with the specified maximum number of bins. This initialized object (dt) can then be used to train the decision tree model on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7136f21f-e3c4-440b-b904-d387bb498d0a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1711380973770,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Initializing a Random Forest model\ndt = DecisionTreeRegressor(\n    featuresCol=\"features\",\n    labelCol=\"Quantity\",\n    maxBins=4000\n)",
    "outputsMetadata": {
     "0": {
      "height": 97,
      "type": "stream"
     },
     "1": {
      "height": 37,
      "type": "stream"
     },
     "2": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Initializing a Random Forest model\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Quantity\",\n",
    "    maxBins=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63dcd6a-3596-48d9-8829-9743523822d6",
   "metadata": {},
   "source": [
    "- ## Create a pipeline for staging the processes\n",
    "\n",
    "**Pipeline**: This initializes a pipeline, which is a sequence of stages used to process and transform data. Stages can include transformations (like indexing and assembling features) and an estimator (like the RandomForestRegressor).\n",
    "\n",
    "**stages=[country_indexer, stock_code_indexer, assembler, rf]**: This parameter specifies the sequence of stages to be executed in the pipeline. It includes the StringIndexer transformations for categorical columns, the VectorAssembler to assemble features, and the RandomForestRegressor as the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb3051ec-c9ac-4c7b-835f-68afa6c0f639",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1711381032746,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create a pipeline for staging the processes\npipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, dt])"
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for staging the processes\n",
    "pipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88539af-ee51-4a79-b1b7-5bc68358c286",
   "metadata": {},
   "source": [
    "- ## Train the Model\n",
    "\n",
    "**pipeline.fit(train_data)**: This method trains the model using the training data. It executes the stages of the pipeline sequentially, transforming the input data and training the RandomForestRegressor model. The resulting model contains the trained Random Forest regression model ready for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a107a3d-ab8e-4b1f-9b74-00007c1fbdcd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2863,
    "lastExecutedAt": 1711381048808,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Training the model\nmodel = pipeline.fit(train_data)",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     },
     "1": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88686e-145f-4885-836b-d27745c66b86",
   "metadata": {},
   "source": [
    "## Evalualting the Model\n",
    "\n",
    "**model.transform(test_data)** : This method applies the trained model to the test dataset (test_data) to make predictions. It uses the pipeline created earlier, which includes all the preprocessing steps (such as feature indexing and vector assembling) and the trained RandomForestRegressor model.\n",
    "\n",
    "**.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))**: This line casts the \"prediction\" column from its original data type to double type. This step ensures consistency in data types, especially if the prediction column was stored as a different data type during the model prediction phase.\n",
    "\n",
    "**test_predictions**: This DataFrame contains the original test dataset along with a new column named \"prediction\", which contains the predicted values generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea390939-36d2-4f28-9435-4cb298a1365a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 926,
    "lastExecutedAt": 1711381631287,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Getting test predictions\ntest_predictions = model.transform(test_data)\ntest_predictions = test_predictions.withColumn(\n    \"prediction\", col(\"prediction\").cast(\"double\"))\n\n# Provide the Mean Absolute Error (MAE) for your forecast? Return a double/floar \"mae\"\n\n# Initializing the evaluator\nmae_evaluator = RegressionEvaluator(\n    labelCol=\"Quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n\n# Obtaining MAE\nmae = mae_evaluator.evaluate(test_predictions)\n\nmae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.940367659308437"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting test predictions\n",
    "test_predictions = model.transform(test_data)\n",
    "test_predictions = test_predictions.withColumn(\n",
    "    \"prediction\", col(\"prediction\").cast(\"double\"))\n",
    "\n",
    "# Provide the Mean Absolute Error (MAE) for your forecast? Return a double/floar \"mae\"\n",
    "\n",
    "# Initializing the evaluator\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "# Obtaining MAE\n",
    "mae = mae_evaluator.evaluate(test_predictions)\n",
    "\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d633e-2910-4b1b-a511-378c8cb1b41d",
   "metadata": {},
   "source": [
    "## Evaluate with MAE Metric\n",
    "\n",
    "**RegressionEvaluator()**: This initializes a RegressionEvaluator object used to evaluate regression models. The labelCol parameter specifies the name of the target variable column (\"Quantity\" in this case), the predictionCol parameter specifies the name of the column containing the predicted values, and the metricName parameter specifies the evaluation metric to be computed (\"mae\" for Mean Absolute Error).\n",
    "\n",
    "**mae_evaluator.evaluate(test_predictions)**: This method computes the Mean Absolute Error (MAE) between the \"Quantity\" column (actual values) and the \"prediction\" column (predicted values) in the test_predictions DataFrame. The resulting mae variable holds the calculated MAE value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fd41a-5f71-4f95-882e-4163885e74bd",
   "metadata": {},
   "source": [
    "_**The evalution procedure will be seen in the [model selection](Selecting the best Model.ipynb) notebook**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5d3cd-0b0d-4895-8a8d-9952d4e4d0da",
   "metadata": {},
   "source": [
    "The MAE value of `8.9` provides insight into the performance of the regression model for forecasting sales quantity. Here are some interpretations of the MAE value and its implications:\n",
    "\n",
    "- Accuracy: The MAE value indicates that, on average, the model's predictions are off by approximately 9.30 units of quantity. This suggests that the model's forecasts are reasonably accurate in predicting the sales quantity.\n",
    "\n",
    "- Error Magnitude: Since MAE measures the absolute difference between the actual and predicted values, a larger MAE value indicates larger errors in the model's predictions. In this case, a MAE of approximately 9.30 suggests that there might be some variability in the predictions, but the model is still making reasonably close predictions on average.\n",
    "\n",
    "- Model Performance: The MAE value serves as a benchmark for evaluating the model's performance. Lower MAE values indicate better predictive accuracy, while higher MAE values suggest poorer performance. Therefore, a MAE of 9.30 suggests that the model is performing fairly well but may have room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dddcb-5006-4f13-a92c-cde23cee13ed",
   "metadata": {},
   "source": [
    "## Identifying the Quantity sold at specific week (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b303f552-7406-4860-a011-f0250d6c4d5b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 676,
    "lastExecutedAt": 1711381650565,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# How many units will be sold during the  week 39 of 2011? Return an integer `quantity_sold_w39`.\n\n# Getting the weekly sales of all countries\nweekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n\n# Finding the quantity sold on the 39 week. \npromotion_week = weekly_test_predictions.filter(col('Week')==39)\n\n# Storing prediction as quantity_sold_w30\nquantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])"
   },
   "outputs": [],
   "source": [
    "# How many units will be sold during the  week 39 of 2011? Return an integer `quantity_sold_w39`.\n",
    "\n",
    "# Getting the weekly sales of all countries\n",
    "weekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n",
    "\n",
    "# Finding the quantity sold on the 39 week. \n",
    "promotion_week = weekly_test_predictions.filter(col('Week')==39)\n",
    "\n",
    "# Storing prediction as quantity_sold_w30\n",
    "quantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00bace-c881-4bdc-ae35-12eb20c797d2",
   "metadata": {},
   "source": [
    "**quantity_sold_w39**: This variable stores the quantity sold during week 39. It does this by selecting the sum of predictions from the promotion_week DataFrame, collecting the result as a list, and then accessing the first element of the first row (assuming there's only one row) to get the sum of predictions for week 39. This sum is converted to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26b28ccb-21c3-45c6-a12d-88c926790a97",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1711381659331,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "quantity_sold_w39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86052"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantity_sold_w39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ffe83-ffc8-48e0-944d-0b34b95441fd",
   "metadata": {},
   "source": [
    "Overall, this code segment effectively extracts the quantity sold during week 39 of 2011 from the predictions made by the regression model. The result is stored in the variable quantity_sold_w39."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adad6a2c-afe5-44f4-ae13-6de20282b9fb",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 89,
    "lastExecutedAt": 1711381728882,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Stop the Spark session\n\nmy_spark.stop()"
   },
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "\n",
    "my_spark.stop()"
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
